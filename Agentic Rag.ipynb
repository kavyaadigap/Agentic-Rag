{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"Open_API_Key\"]=os.getenv(\"Open_API_Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f798ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8d88460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kavyasree P\\AppData\\Local\\Temp\\ipykernel_26632\\1152939508.py:9: LangChainDeprecationWarning: Default values for HuggingFaceBgeEmbeddings.model_name were deprecated in LangChain 0.2.5 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceBgeEmbeddings constructor instead.\n",
      "  vectorstore = FAISS.from_documents(documents=doc_splits, embedding=HuggingFaceBgeEmbeddings())\n"
     ]
    }
   ],
   "source": [
    "urls=[\"https://www.oreilly.com/library/view/learning-python-5th/9781449356862/\",\"https://www.oreilly.com/library/view/python-cookbook-3rd/9781449340373/\",\"https://www.oreilly.com/library/view/python-in-a-nutshell/9781449357333/\"]\n",
    "docs=[]\n",
    "for url in urls:\n",
    "    docs.append(WebBaseLoader(url).load())\n",
    "docss_list =[item for sublist in docs for item in sublist]  # Flatten the list of lists\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "doc_splits = text_splitter.split_documents(docss_list)\n",
    "vectorstore = FAISS.from_documents(documents=doc_splits, embedding=HuggingFaceBgeEmbeddings())\n",
    "retrieverpython= vectorstore.as_retriever()\n",
    "\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool_python = create_retriever_tool(\n",
    "    retrieverpython,\n",
    "    \"retriever_vector_python\",\n",
    "    \"A tool to retrieve Python-related information from a vector store.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "886915ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kavyasree P\\AppData\\Local\\Temp\\ipykernel_26632\\1400811609.py:13: LangChainDeprecationWarning: Default values for HuggingFaceBgeEmbeddings.model_name were deprecated in LangChain 0.2.5 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceBgeEmbeddings constructor instead.\n",
      "  vectorstore = FAISS.from_documents(documents=doc_splits, embedding=HuggingFaceBgeEmbeddings())\n"
     ]
    }
   ],
   "source": [
    "lang_chain_urls = [\n",
    "    \"https://python.langchain.com/docs/get_started/introduction.html\",\n",
    "    \"https://python.langchain.com/docs/get_started/quickstart.html\",\n",
    "    \"https://python.langchain.com/docs/get_started/faq.html\",\n",
    "]\n",
    "docs_langchain = []\n",
    "for url in lang_chain_urls:\n",
    "    docs_langchain.append(WebBaseLoader(url).load())\n",
    "docss_list = [item for sublist in docs_langchain for item in sublist]  # Flatten the list of lists\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "doc_splits = text_splitter.split_documents(docss_list)\n",
    "vectorstore = FAISS.from_documents(documents=doc_splits, embedding=HuggingFaceBgeEmbeddings())\n",
    "retrieverlangchain = vectorstore.as_retriever()\n",
    "\n",
    "retrieval_tool_langchain = create_retriever_tool(\n",
    "    retrieverlangchain,\n",
    "    \"retriever_vector_langchain\",\n",
    "    \"A tool to retrieve LangChain-related information from a vector store.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f820f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools= [retriever_tool_python,retrieval_tool_langchain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cbbe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langgraph\n",
    "\n",
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_message\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_message]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c93aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    model=\"groq-llama-3.1-70b-chat\")\n",
    "llm.invoke(\"What is LangChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aac3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    print(\"___CALLING AGENT ___\")\n",
    "    message=state[\"message\"]\n",
    "    model= ChatGroq(\n",
    "        model=\"groq-llama-3.1-70b-chat\",\n",
    "    )   \n",
    "    model=model.bind_tools(tools)\n",
    "    response=model.invoke(message)\n",
    "    return {\"message\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05d32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a09326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state) -> Literal[\"generate\",\"rewrite\"]:\n",
    "    '''Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "Args :\n",
    "state (messages): The current state\n",
    "\n",
    "Returns :\n",
    "str: A decision for whether the documents are relevant or not'''\n",
    "    print(\"___CHECKING RELEVANCE ___\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check. \"\"\"\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatGroq(model=\"qwen-qwq-32b\")\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance of a retrieved document to a user. Here is the retrieved documeht: \\n\\n {context} \\n\\n\n",
    "    Here is the user question: {question} \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the user questior\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant\"\"\"\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf54229",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(\"What is LangChain?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
